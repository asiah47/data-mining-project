## Data Mining Assignment 

ASIA HUSSEIN 
I WORKED AND SUBMITTED ALONE 

OVERVIEW 
This repository contains my solution to the Data Mining module's assignment. The assignment consists of two parts. Since I am working alone, 
I have completed Part 1(a) and implemented three algorithms from Part 2, as per the assignment guidelines.

FILES INCLUDE: 
PART 1:
sonar_train.csv
sonar_test.csv
Part1_Asia.ipynb 

PART 2:
creditdefault_train.csv 
creditdefault_test.csv
Part2_Asia.ipynb


ASSIGNMENT BREIF / TASK DESCRIPTION 

PART 1:

This task is based on the Sonar real data seen previously in class. Several objects which can be rock or metal cylinders are scanned on different 
angles and under different conditions, with sonar signals. 60 measurements are recorded per columns for each object (one record per object) and 
these are the predictors called A1, A2, â€¦, A60. The label associated with each record contains the letter "R" if the object is a rock and "M" if 
it is metal cylinder, and this is the outcome variable called Class.

Two datasets are provided to you: a training dataset in the sonar_train.csv file, and a test dataset in the sonar_test.csv file.

a) You are required to write a Python code implementing the simple Nearest Neighbour algorithm, with the Minkowski distance, both discussed in 
lecture of week 1. You should not implement k-Nearest Neighbour, for an arbitrary number of neighbours k. Your code will read the power q 
appearing in the Mionkowski distance, and will classify each record from the test dataset based on the training dataset. Remember, to classify a 
record from the test set you need to find its nearest neighbour in the training set (this is the one which minimizes the distance to the test 
set record); take the class of the nearest neighbour as the predicted class for the test set record. After classifying all the records in the 
test set, your code needs to calculate and display the accuracy, recall, precision, and F1 measure with respect to the class "M" (which is 
assumed to be the positive class), of the predictions on the test dataset. Run your code to produce results for Manhattan and for Euclidian 
distances, which are particular cases of Minkowski's distance.

b) Run your code for the power q as a positive integer number from 1 to 20 and display the accuracy, recall, precision, and F1 measure on the 
test set in a chart. Which value of q leads to the best accuracy on the test set?

The code, comments, explanations and results will be provided in a Jupyter notebook called Part1.

Note that in this task you are not to apply a library for the nearest neighbour algorithm, but you are required to compute the distances, find 
the nearest neighbour, and so code yourself this simple algorithm. 

PART 2:

This task is based on a real credit risk data, and is to predict a response variable Y which represents a credit card default payment (Yes = 1, 
No = 0), using the 23 input variables as follows:

X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit.
X2: Gender (1 = male; 2 = female).
X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).
X4: Marital status (1 = married; 2 = single; 3 = others).
X5: Age (year).
X6 - X11: History of past payment. One tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the repayment status in September, 2005; X7 = the repayment status in August, 2005; . . .;X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.
X12-X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; . . .; X17 = amount of bill statement in April, 2005.
X18-X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount paid in April, 2005.

Two datasets are provided to you: a training dataset in the creditdefault_train.csv file, and a test dataset in the creditdefault_test.csv file.

Using Python and any relevant libraries, you are required to build the best predictive model by tuning models using cross validation on the 
training dataset with each of the following algorithms discussed in this module: k-Nearest Neighbours, Decision Trees, Random Forest, Bagging, 
AdaBoost, and SVM. You may replace the AdaBoost algorithm with XGBoost algorithm, but do not use both of them. Out of the models tuned with all 
the above algorithms, select the best model and clearly justify your choice of performance criteria, and evaluate its performances on the test 
set. 

Bonus points for this Part 2: there are up to 6 bonus points if you incorporate in your analysis the computation of an optimal threshold (as for 
instance probability threshold) for the classification for your best model.

The coding, comments and explanations will be provided in your Python Jupyter notebook called Part2, which should include also the results. 
Moreover, for each algorithm mentioned above, include 1 chart in the notebook illustrating how accuracy of the models vary when you vary the 
values of one numeric hyperparameter only (at your choice).





